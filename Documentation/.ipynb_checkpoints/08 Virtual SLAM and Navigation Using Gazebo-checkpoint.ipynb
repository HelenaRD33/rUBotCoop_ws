{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual SLAM and Navigation Using Gazebo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SLAM (short for Simultaneous Localization and Mapping) techniques, you will be able to execute autonomous navigation with GoPiGo3.\n",
    "\n",
    "SLAM is a technique used in robotics to explore and map an unknown environment while estimating the pose of the robot itself. As it moves all around, it will be acquiring structured information of the surroundings by processing the raw data coming from its sensors.\n",
    "\n",
    "For optimal and easy-to-understand coverage of the topic of SLAM, we will implement a 360º-coverage Laser Distance Sensor (LDS) in the virtual robot. There are low-cost versions of this sensor technology, such as EAI YDLIDAR X4 (available at https://www.aliexpress.com/item/32908156152.html), which is the one we will make use of in the next chapter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy its files to the ROS workspace to have them available, and leave the rest outside of the src folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp -R ~/Hands-On-ROS-for-Robotics-Programming/Chapter8_Virtual_SLAM ~/rUBotCoop_ws/src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code contains two new ROS packages as follows: \n",
    "- gopigo3_description, which contains the URDF model plus the SDF (Gazebo tags) for a complete, dynamic simulation. This package provides the gopigo3_rviz.launch launch file to interactively visualize the model in RViz.\n",
    "- virtual_slam contains the virtual robot simulation itself, plus the launch files needed to run SLAM in Gazebo.\n",
    "\n",
    "Then, rebuild the workspace so that it is known to your ROS installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/rUBotCoop_ws\n",
    "catkin_make "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROS navigation packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's prepare your machine with the required ROS packages needed for the navigation stack:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt install ros-melodic-navigation ros-melodic-amcl ros-melodic-map-server ros-melodic-move-base ros-melodic-urdf ros-melodic-xacro ros-melodic-compressed-image-transport ros-melodic-rqt-image-view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the slam_gmapping package, that the time of writing is already available in its binary version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install ros-melodic-slam-gmapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding sensors to the GoPiGo3 model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, you should have equipped your virtual robot with a differential drive controller that provides the capability to convert velocity commands into rotations of the left and right wheels. We need to complete the model with some sort of perception of the environment. For this, we will add controllers for two common sensors, a two-dimensional camera and an LDS. The first corresponds to the Pi camera of your physical robot, while the second is the unidirectional distance sensor of the GoPiGo3 kit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add the solid of the camera as usual with <visual> tags, but since it is a commercial device, you can a get better look by using a realistic three-dimensional CAD model supplied by the manufacturer or made by someone else in the open source community. The URDF definition is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<link name=\"camera\">\n",
    "  <visual>\n",
    "    <origin xyz=\"0.25 0 0.05\" rpy=\"0 1.570795 0\" />\n",
    "    <geometry>\n",
    "      <mesh filename=\"package://virtual_slam/meshes/piCamera.stl\" scale=\"0.5 0.5 0.5\"/>\n",
    "    </geometry>\n",
    "  </visual>\n",
    "...\n",
    "</link>\n",
    "<joint name=\"joint_camera\" type=\"fixed\">\n",
    "    <parent link=\"base_link\"/>\n",
    "    <child link=\"camera\"/>\n",
    "    <origin xyz=\"0 0 0\" rpy=\"0 0 0\" /> \n",
    "    <axis xyz=\"1 0 0\" />\n",
    "</joint>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the camera technical features using a <gazebo> tag that emulates the behavior of the camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<gazebo reference=\"camera\">\n",
    "  <sensor type=\"camera\" name=\"camera1\">\n",
    "    <update_rate>30.0</update_rate>\n",
    "    <camera name=\"front\">\n",
    "      <horizontal_fov>1.3962634</horizontal_fov>\n",
    "      <image>\n",
    "        <width>800</width>\n",
    "        <height>800</height>\n",
    "        <format>R8G8B8</format>\n",
    "      </image>\n",
    "    <clip>\n",
    "      <near>0.02</near>\n",
    "      <far>300</far>\n",
    "    </clip>\n",
    "    </camera>\n",
    "    <!-- plugin \"camera_controller\" filename=\"libgazebo_ros_camera.so\" -->\n",
    "  </sensor>\n",
    "</gazebo>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "camera images will be published in the /gopigo/camera1/image_raw topic.\n",
    "\n",
    "Launch the ROS visualization tool to check that the model is properly built. Since RViz only represents its visual features—it does not include any physical simulation engine—it is a much lighter environment than Gazebo and you have available all the options to check every aspect of the appearance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch gopigo3_description gopigo3_basic_rviz.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_model_sensors1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
