{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual SLAM and Navigation Using Gazebo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SLAM (short for Simultaneous Localization and Mapping) techniques, you will be able to execute autonomous navigation with GoPiGo3.\n",
    "\n",
    "SLAM is a technique used in robotics to explore and map an unknown environment while estimating the pose of the robot itself. As it moves all around, it will be acquiring structured information of the surroundings by processing the raw data coming from its sensors.\n",
    "\n",
    "For optimal and easy-to-understand coverage of the topic of SLAM, we will implement a 360º-coverage Laser Distance Sensor (LDS) in the virtual robot. There are low-cost versions of this sensor technology, such as EAI YDLIDAR X4 (available at https://www.aliexpress.com/item/32908156152.html), which is the one we will make use of in the next chapter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy its files to the ROS workspace to have them available, and leave the rest outside of the src folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp -R ~/Hands-On-ROS-for-Robotics-Programming/Chapter8_Virtual_SLAM ~/rUBotCoop_ws/src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code contains two new ROS packages as follows: \n",
    "- gopigo3_description, which contains the URDF model plus the SDF (Gazebo tags) for a complete, dynamic simulation. This package provides the gopigo3_rviz.launch launch file to interactively visualize the model in RViz.\n",
    "- virtual_slam contains the virtual robot simulation itself, plus the launch files needed to run SLAM in Gazebo.\n",
    "\n",
    "Then, rebuild the workspace so that it is known to your ROS installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/rUBotCoop_ws\n",
    "catkin_make "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROS navigation packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's prepare your machine with the required ROS packages needed for the navigation stack:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt install ros-melodic-navigation ros-melodic-amcl ros-melodic-map-server ros-melodic-move-base ros-melodic-urdf ros-melodic-xacro ros-melodic-compressed-image-transport ros-melodic-rqt-image-view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the slam_gmapping package, that the time of writing is already available in its binary version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install ros-melodic-slam-gmapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding sensors to the GoPiGo3 model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, you should have equipped your virtual robot with a differential drive controller that provides the capability to convert velocity commands into rotations of the left and right wheels. We need to complete the model with some sort of perception of the environment. For this, we will add controllers for two common sensors, a two-dimensional camera and an LDS. The first corresponds to the Pi camera of your physical robot, while the second is the unidirectional distance sensor of the GoPiGo3 kit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add the solid of the camera as usual with <visual> tags, but since it is a commercial device, you can a get better look by using a realistic three-dimensional CAD model supplied by the manufacturer or made by someone else in the open source community. The URDF definition is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<link name=\"camera\">\n",
    "  <visual>\n",
    "    <origin xyz=\"0.25 0 0.05\" rpy=\"0 1.570795 0\" />\n",
    "    <geometry>\n",
    "      <mesh filename=\"package://virtual_slam/meshes/piCamera.stl\" scale=\"0.5 0.5 0.5\"/>\n",
    "    </geometry>\n",
    "  </visual>\n",
    "...\n",
    "</link>\n",
    "<joint name=\"joint_camera\" type=\"fixed\">\n",
    "    <parent link=\"base_link\"/>\n",
    "    <child link=\"camera\"/>\n",
    "    <origin xyz=\"0 0 0\" rpy=\"0 0 0\" /> \n",
    "    <axis xyz=\"1 0 0\" />\n",
    "</joint>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the camera technical features using a <gazebo> tag that emulates the behavior of the camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<gazebo reference=\"camera\">\n",
    "  <sensor type=\"camera\" name=\"camera1\">\n",
    "    <update_rate>30.0</update_rate>\n",
    "    <camera name=\"front\">\n",
    "      <horizontal_fov>1.3962634</horizontal_fov>\n",
    "      <image>\n",
    "        <width>800</width>\n",
    "        <height>800</height>\n",
    "        <format>R8G8B8</format>\n",
    "      </image>\n",
    "    <clip>\n",
    "      <near>0.02</near>\n",
    "      <far>300</far>\n",
    "    </clip>\n",
    "    </camera>\n",
    "    <!-- plugin \"camera_controller\" filename=\"libgazebo_ros_camera.so\" -->\n",
    "  </sensor>\n",
    "</gazebo>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "camera images will be published in the /gopigo/camera1/image_raw topic.\n",
    "\n",
    "Launch the ROS visualization tool to check that the model is properly built. Since RViz only represents its visual features—it does not include any physical simulation engine—it is a much lighter environment than Gazebo and you have available all the options to check every aspect of the appearance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch gopigo3_description gopigo3_basic_rviz.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_model_sensors1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first place the robot in Gazebo the same way we did in the previous chapter and enable remote control with the keyboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch virtual_slam gopigo3_basic_world.launch\n",
    "rosrun key_teleop key_teleop.py /key_vel:=/cmd_vel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "key_teleop allows you to remotely control the GoPiGo3 with the arrow keys of your keyboard.\n",
    "\n",
    "Now, launch a node from the image_view package that comes preinstalled with ROS (we are remapping the image topic so that the node takes its data from the camera node topic, /gopigo/camera1/image_raw):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rosrun image_view image_view image:=/gopigo/camera1/image_raw "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teleoperate the robot with the arrow keys and you will see the subjective view in the image window:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_Gazebo_Cam1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's obtain the ROS graph with the well-known command, rqt_graph, and have a look at how the topic remapping for the image is handled:\n",
    "\n",
    "Thanks to the mapping argument, image:=/gopigo/camera1/image_raw, the image topic of the image_view package remains implicit and just the /gopigo/camera1/image_raw is visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_Gazebo_rqt_cam.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First to the previous Gazebo process, type:\n",
    "\n",
    "killall gzserver && killall gzclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the solid model of this sensor under the <visual> tag by following the same procedure we covered for the camera. The URDF definition is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<joint name=\"distance_sensor_solid_joint\" type=\"fixed\">\n",
    "    <axis xyz=\"0 1 0\" />\n",
    "    <origin rpy=\"0 0 0\" xyz=\"0 0 0\" />\n",
    "    <parent link=\"base_link\"/>\n",
    "    <child link=\"distance_sensor_solid\"/>\n",
    "</joint>\n",
    "<link name=\"distance_sensor_solid\">\n",
    "    <visual>\n",
    "      <origin xyz=\"0.2 0 0.155\" rpy=\"1.570795 0 1.570795\" />\n",
    "      <geometry>\n",
    "        <mesh filename=\"package://gopigo3_description/meshes/IR_Sensor_Sharp_GP2Y_solid.stl\" scale=\"0.005 0.005 0.005\"/>\n",
    "      </geometry>\n",
    "      <material name=\"red\"/>\n",
    "    </visual>\n",
    "    ...\n",
    "</link>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the sensor technical features using a <gazebo> tag, which you can see refers to the distance_sensor link defined in the preceding snippet (not distance_sensor_solid):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<gazebo reference=\"distance_sensor\"> \n",
    "   <sensor type=\"ray\" name=\"laser_distance\">\n",
    "      <visualize>true</visualize>\n",
    "      <update_rate>10</update_rate>\n",
    "      <ray>\n",
    "         ...\n",
    "         <range>\n",
    "            <min>0.01</min>\n",
    "            <max>3</max>\n",
    "            <resolution>0.01</resolution>\n",
    "         </range>\n",
    "      </ray>\n",
    "      <!-- plugin filename=\"libgazebo_ros_range.so\" name=\"gazebo_ros_ir\" -->\n",
    "    </sensor> \n",
    "   </gazebo>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <update_rate> tag specifies that the sensor is read at a frequency of 10 Hz, and the <range> tag sets measured distance values between 10 cm and 3 m at 1 cm resolution.\n",
    "\n",
    "Finally, we add the Gazebo plugin that emulates the behavior of the distance sensor. The following snippet is what substitutes the commented line referring to plugin \"gazebo_ros_ir\" in the preceding code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      <plugin filename=\"libgazebo_ros_range.so\" name=\"gazebo_ros_ir\">\n",
    "         <gaussianNoise>0.005</gaussianNoise>\n",
    "         <alwaysOn>true</alwaysOn>\n",
    "         <updateRate>0.0</updateRate>\n",
    "             <topicName>gopigo/distance_sensor</topicName>\n",
    "             <frameName>distance_sensor</frameName>\n",
    "         <radiation>INFRARED</radiation>\n",
    "         <fov>0.02</fov>\n",
    "       </plugin>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the ROS visualization tool to check that the model is properly built. Since RViz only represents its visual features, it is a much lighter environment than Gazebo and you have available all the options to check every aspect of the appearance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch gopigo3_description gopigo3_basic_rviz.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_model_sensors1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test includes both the distance sensor and the two-dimensional camera. Run the example by using four Terminals, as indicated in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch virtual_slam gopigo3_basic_world.launch\n",
    "rosrun key_teleop key_teleop.py /key_vel:=/cmd_vel\n",
    "rostopic echo /gopigo/distance\n",
    "rosrun image_view image_view image:=/gopigo/camera1/image_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_Gazebo_distance_sensor1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First to the previous Gazebo process, type:\n",
    "\n",
    "killall gzserver && killall gzclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laser Distance Sensor (LDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the solid model of this sensor under the <visual> tag by following the same procedure we covered for the previous sensors. The URDF definition is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<link name=\"base_scan\">\n",
    "    <visual name=\"sensor_body\">\n",
    "      <origin xyz=\"0 0 0\" rpy=\"0 0 0\" />\n",
    "      <geometry>\n",
    "        <mesh filename=\"package://gopigo3_description/meshes/TB3_lds-01.stl\" scale=\"0.003 0.003 0.003\"/> \n",
    "      </geometry>\n",
    "      <material name=\"yellow\"/>\n",
    "    </visual>\n",
    "    <visual name=\"support\">\n",
    "      <origin xyz=\"0 0 -0.0625\" rpy=\"0 0 0\" />\n",
    "      <geometry>\n",
    "        <cylinder length=\"0.12\" radius=\"0.1\" />\n",
    "      </geometry>\n",
    "    </visual> \n",
    "</link>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see two <visual> blocks within the <link> element in the preceding snippet: sensor_body is the LDS itself, and support creates the physical interface between the sensor and the robot chassis. The solid model that we are using for the sensor body is the one shown in the following screenshot, which consists of a CAD model in STL format referenced from the <mesh> tag.\n",
    "    \n",
    "Next, we add a <joint> element of <type=\"fixed\"> to attach the sensor assembly to the robot chassis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<joint name=\"scan_joint\" type=\"fixed\">\n",
    "    <parent link=\"base_link\"/>\n",
    "    <child link=\"base_scan\"/>\n",
    "    <origin xyz=\"-0.1 0 0.25\" rpy=\"0 0 0\"/>\n",
    "</joint>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the sensor technical features using a <gazebo> tag that you can see refers to the distance_sensor link defined in the preceding snippet (not distance_sensor_solid):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<gazebo reference=\"base_scan\">\n",
    "    <sensor type=\"ray\" name=\"lds_lfcd_sensor\">\n",
    "      <visualize>true</visualize>\n",
    "      <update_rate>5</update_rate>\n",
    "      <ray>\n",
    "        <scan>\n",
    "          <horizontal> <samples>721</samples> ... </horizontal>\n",
    "        </scan>\n",
    "        <range>\n",
    "          <min>0.12</min>\n",
    "          <max>10</max>\n",
    "          <resolution>0.015</resolution>\n",
    "        </range>\n",
    "      </ray>\n",
    "        <!-- plugin name=\"gazebo_ros_lds_lfcd_controller\" filename=\"libgazebo_ros_laser.so\" -->\n",
    "    </sensor>\n",
    "  </gazebo>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <range> tag sets measured distance values between 12 cm and 10 m, as can be found in the technical specification of the EAI YDLIDAR X4. Pay special attention to the <visualize>true</visualize> tag, since, with a sensor like this, with 360º vision, the screen will be filled with rays to show the angle range that it covers. It is recommended to set this to false once you have visually checked that the sensor is working properly.\n",
    "    \n",
    "The <update_rate> tag specifies that the sensor is read at a frequency of 5 Hz, but the specification of the LDS is 5,000 Hz. Why don't we put the actual value? Since the robot will move at low speed, there is no need to have such a high-frequency reading, so we can limit it to only 5 Hz, which will have no impact on the robot behavior. This will require only 55 Kb/s of bandwidth, 1,000 times lower than what the sensor can provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to add the Gazebo plugin that emulates the behavior of the distance sensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<plugin name=\"gazebo_ros_lds_lfcd_controller\" filename=\"libgazebo_ros_laser.so\">\n",
    "        <topicName>/gopigo/scan</topicName>\n",
    "        <frameName>base_scan</frameName>\n",
    "</plugin>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the range values will be published in the /gopigo/scan topic.\n",
    "\n",
    "Finally, launch the ROS visualization tool to check that the model is properly built. Since RViz only represents its visual features, it is a much lighter environment than Gazebo and you have available all the options to check every aspect of the appearance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch gopigo3_description gopigo3_rviz.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_full_model_rviz.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After including the LDS model in the virtual robot, we can proceed to see how it works by running the simulation in Gazebo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch virtual_slam gopigo3_world.launch\n",
    "rosrun key_teleop key_teleop.py /key_vel:=/cmd_vel\n",
    "rostopic echo /scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_gazebo_lidar1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this sensor, it is better to use a Python script that makes the robot wander in the environment while avoiding the obstacles. To do this, we have implemented the following rules in our script: \n",
    "- If there is no obstacle, move forward at a reference speed of 0.8 m/s. \n",
    "- If the range provided by the distance sensor is lower than 2 meters, go back and rotate counter-clockwise until avoiding the obstacle. \n",
    "- Since the distance sensor throws unidirectional measurements, we should check the measurements from the LDS to find if there are obstacles to the sides, and the threshold should be lower than 1.6 meters. If obstacles are detected, go back and rotate counter-clockwise faster to avoid the obstacle and not get stuck on it.\n",
    "\n",
    "This simple algorithm is implemented in the wanderAround.py script, and can be found under the ./virtual_slam/scripts/wanderAround.py folder.\n",
    "\n",
    "First Kill the previous Gazebo process:\n",
    "killall gzserver && killall gzclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch virtual_slam gopigo3_world.launch\n",
    "rosrun virtual_slam wanderAround.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_gazebo_lidar2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kill the previous Gazebo process, type:\n",
    "killall gzserver && killall gzclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
