{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual SLAM and Navigation Using Gazebo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SLAM (short for Simultaneous Localization and Mapping) techniques, you will be able to execute autonomous navigation with GoPiGo3.\n",
    "\n",
    "SLAM is a technique used in robotics to explore and map an unknown environment while estimating the pose of the robot itself. As it moves all around, it will be acquiring structured information of the surroundings by processing the raw data coming from its sensors.\n",
    "\n",
    "For optimal and easy-to-understand coverage of the topic of SLAM, we will implement a 360º-coverage Laser Distance Sensor (LDS) in the virtual robot. There are low-cost versions of this sensor technology, such as EAI YDLIDAR X4 (available at https://www.aliexpress.com/item/32908156152.html), which is the one we will make use of in the next chapter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy its files to the ROS workspace to have them available, and leave the rest outside of the src folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp -R ~/Hands-On-ROS-for-Robotics-Programming/Chapter8_Virtual_SLAM ~/rUBotCoop_ws/src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code contains two new ROS packages as follows: \n",
    "- gopigo3_description, which contains the URDF model plus the SDF (Gazebo tags) for a complete, dynamic simulation. This package provides the gopigo3_rviz.launch launch file to interactively visualize the model in RViz.\n",
    "- virtual_slam contains the virtual robot simulation itself, plus the launch files needed to run SLAM in Gazebo.\n",
    "\n",
    "Then, rebuild the workspace so that it is known to your ROS installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/rUBotCoop_ws\n",
    "catkin_make "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROS navigation packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's prepare your machine with the required ROS packages needed for the navigation stack:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt install ros-melodic-navigation ros-melodic-amcl ros-melodic-map-server ros-melodic-move-base ros-melodic-urdf ros-melodic-xacro ros-melodic-compressed-image-transport ros-melodic-rqt-image-view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the slam_gmapping package, that the time of writing is already available in its binary version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install ros-melodic-slam-gmapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding sensors to the GoPiGo3 model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, you should have equipped your virtual robot with a differential drive controller that provides the capability to convert velocity commands into rotations of the left and right wheels. We need to complete the model with some sort of perception of the environment. For this, we will add controllers for two common sensors, a two-dimensional camera and an LDS. The first corresponds to the Pi camera of your physical robot, while the second is the unidirectional distance sensor of the GoPiGo3 kit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add the solid of the camera as usual with <visual> tags, but since it is a commercial device, you can a get better look by using a realistic three-dimensional CAD model supplied by the manufacturer or made by someone else in the open source community. The URDF definition is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<link name=\"camera\">\n",
    "  <visual>\n",
    "    <origin xyz=\"0.25 0 0.05\" rpy=\"0 1.570795 0\" />\n",
    "    <geometry>\n",
    "      <mesh filename=\"package://virtual_slam/meshes/piCamera.stl\" scale=\"0.5 0.5 0.5\"/>\n",
    "    </geometry>\n",
    "  </visual>\n",
    "...\n",
    "</link>\n",
    "<joint name=\"joint_camera\" type=\"fixed\">\n",
    "    <parent link=\"base_link\"/>\n",
    "    <child link=\"camera\"/>\n",
    "    <origin xyz=\"0 0 0\" rpy=\"0 0 0\" /> \n",
    "    <axis xyz=\"1 0 0\" />\n",
    "</joint>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the camera technical features using a <gazebo> tag that emulates the behavior of the camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<gazebo reference=\"camera\">\n",
    "  <sensor type=\"camera\" name=\"camera1\">\n",
    "    <update_rate>30.0</update_rate>\n",
    "    <camera name=\"front\">\n",
    "      <horizontal_fov>1.3962634</horizontal_fov>\n",
    "      <image>\n",
    "        <width>800</width>\n",
    "        <height>800</height>\n",
    "        <format>R8G8B8</format>\n",
    "      </image>\n",
    "    <clip>\n",
    "      <near>0.02</near>\n",
    "      <far>300</far>\n",
    "    </clip>\n",
    "    </camera>\n",
    "    <!-- plugin \"camera_controller\" filename=\"libgazebo_ros_camera.so\" -->\n",
    "  </sensor>\n",
    "</gazebo>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "camera images will be published in the /gopigo/camera1/image_raw topic.\n",
    "\n",
    "Launch the ROS visualization tool to check that the model is properly built. Since RViz only represents its visual features—it does not include any physical simulation engine—it is a much lighter environment than Gazebo and you have available all the options to check every aspect of the appearance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch gopigo3_description gopigo3_basic_rviz.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_model_sensors1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first place the robot in Gazebo the same way we did in the previous chapter and enable remote control with the keyboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch virtual_slam gopigo3_basic_world.launch\n",
    "rosrun key_teleop key_teleop.py /key_vel:=/cmd_vel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "key_teleop allows you to remotely control the GoPiGo3 with the arrow keys of your keyboard.\n",
    "\n",
    "Now, launch a node from the image_view package that comes preinstalled with ROS (we are remapping the image topic so that the node takes its data from the camera node topic, /gopigo/camera1/image_raw):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rosrun image_view image_view image:=/gopigo/camera1/image_raw "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teleoperate the robot with the arrow keys and you will see the subjective view in the image window:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_Gazebo_Cam1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's obtain the ROS graph with the well-known command, rqt_graph, and have a look at how the topic remapping for the image is handled:\n",
    "\n",
    "Thanks to the mapping argument, image:=/gopigo/camera1/image_raw, the image topic of the image_view package remains implicit and just the /gopigo/camera1/image_raw is visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_Gazebo_rqt_cam.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the solid model of this sensor under the <visual> tag by following the same procedure we covered for the camera. The URDF definition is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<joint name=\"distance_sensor_solid_joint\" type=\"fixed\">\n",
    "    <axis xyz=\"0 1 0\" />\n",
    "    <origin rpy=\"0 0 0\" xyz=\"0 0 0\" />\n",
    "    <parent link=\"base_link\"/>\n",
    "    <child link=\"distance_sensor_solid\"/>\n",
    "</joint>\n",
    "<link name=\"distance_sensor_solid\">\n",
    "    <visual>\n",
    "      <origin xyz=\"0.2 0 0.155\" rpy=\"1.570795 0 1.570795\" />\n",
    "      <geometry>\n",
    "        <mesh filename=\"package://gopigo3_description/meshes/IR_Sensor_Sharp_GP2Y_solid.stl\" scale=\"0.005 0.005 0.005\"/>\n",
    "      </geometry>\n",
    "      <material name=\"red\"/>\n",
    "    </visual>\n",
    "    ...\n",
    "</link>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the sensor technical features using a <gazebo> tag, which you can see refers to the distance_sensor link defined in the preceding snippet (not distance_sensor_solid):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<gazebo reference=\"distance_sensor\"> \n",
    "   <sensor type=\"ray\" name=\"laser_distance\">\n",
    "      <visualize>true</visualize>\n",
    "      <update_rate>10</update_rate>\n",
    "      <ray>\n",
    "         ...\n",
    "         <range>\n",
    "            <min>0.01</min>\n",
    "            <max>3</max>\n",
    "            <resolution>0.01</resolution>\n",
    "         </range>\n",
    "      </ray>\n",
    "      <!-- plugin filename=\"libgazebo_ros_range.so\" name=\"gazebo_ros_ir\" -->\n",
    "    </sensor> \n",
    "   </gazebo>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <update_rate> tag specifies that the sensor is read at a frequency of 10 Hz, and the <range> tag sets measured distance values between 10 cm and 3 m at 1 cm resolution.\n",
    "\n",
    "Finally, we add the Gazebo plugin that emulates the behavior of the distance sensor. The following snippet is what substitutes the commented line referring to plugin \"gazebo_ros_ir\" in the preceding code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      <plugin filename=\"libgazebo_ros_range.so\" name=\"gazebo_ros_ir\">\n",
    "         <gaussianNoise>0.005</gaussianNoise>\n",
    "         <alwaysOn>true</alwaysOn>\n",
    "         <updateRate>0.0</updateRate>\n",
    "             <topicName>gopigo/distance_sensor</topicName>\n",
    "             <frameName>distance_sensor</frameName>\n",
    "         <radiation>INFRARED</radiation>\n",
    "         <fov>0.02</fov>\n",
    "       </plugin>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the ROS visualization tool to check that the model is properly built. Since RViz only represents its visual features, it is a much lighter environment than Gazebo and you have available all the options to check every aspect of the appearance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch gopigo3_description gopigo3_basic_rviz.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_model_sensors1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test includes both the distance sensor and the two-dimensional camera. Run the example by using four Terminals, as indicated in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roslaunch virtual_slam gopigo3_basic_world.launch\n",
    "rosrun key_teleop key_teleop.py /key_vel:=/cmd_vel\n",
    "rostopic echo /gopigo/distance\n",
    "rosrun image_view image_view image:=/gopigo/camera1/image_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/08_Gazebo_distance_sensor1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
